{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAQqFQfSo66d"
   },
   "source": [
    "### Michael Li (ml5803) and Kaixuan Zhou (kz1005) <br/>\n",
    "### Neural Network\n",
    "### Text Classification : Toxic, Information, Sports, Religious, and Advertisement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SD4ero9bqG_v"
   },
   "source": [
    "# Initialization and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSCjN9vt_GIE"
   },
   "outputs": [],
   "source": [
    "#Let's ignore the warnings...\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip3NO64__b-3"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import gspread\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26355,
     "status": "ok",
     "timestamp": 1575857577736,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "ko7wVt7k_dMk",
    "outputId": "b4d380e7-3881-4f5b-a537-1d24f5222d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Handle', 'TweetID', 'Tweet', 'Toxic', 'Information', 'Sports', 'Religious', 'Advertisement', 'Classification']\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "worksheet = gc.open('output_copy').sheet1\n",
    "records = 30000\n",
    "rows = worksheet.get_all_values()\n",
    "header = rows[0]\n",
    "data = rows[1:records]\n",
    "\n",
    "print(rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TA9XDbVJLew2"
   },
   "outputs": [],
   "source": [
    "#Uncomment if you want the dataset to be balanced\n",
    "\n",
    "# balanced_data = []\n",
    "# count = [0, 0, 0, 0, 0]\n",
    "# for tweet in data:\n",
    "#     count[int(tweet[-1])] += 1\n",
    "\n",
    "# new_count = [0, 0, 0, 0, 0]\n",
    "# min_count = min(count)\n",
    "# for tweet in data:\n",
    "#     if new_count[int(tweet[-1])] < min_count:\n",
    "#         balanced_data.append(tweet)\n",
    "#         new_count[int(tweet[-1])] += 1\n",
    "\n",
    "# data = balanced_data\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26345,
     "status": "ok",
     "timestamp": 1575857577737,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "wXnfKGfaAEJK",
    "outputId": "c1239425-dad2-4172-ac60-d15efb2ccee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14076, 9) (7039, 9)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = header) \n",
    "#shuffle df\n",
    "df = df.sample(frac= 1).reset_index(drop=True)\n",
    "\n",
    "np_arr = np.array(df)\n",
    "\n",
    "train_ind = int(len(data) // 1.5)\n",
    "df_train = df[:train_ind]\n",
    "df_test = df[train_ind:]\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGi7UBHdqZtc"
   },
   "source": [
    "# Setup and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27663,
     "status": "ok",
     "timestamp": 1575857579063,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "YBfvZcPuBH98",
    "outputId": "74d42e04-bd24-48ae-eb2d-a65c52f80a6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s):\n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yJV1p70q-YD"
   },
   "source": [
    "# Code for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNbk3qP5ARi7"
   },
   "outputs": [],
   "source": [
    "# Create mapping from word to vector\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "trn_term_doc = vec.fit_transform(df_train[\"Tweet\"])\n",
    "test_term_doc = vec.transform(df_test[\"Tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29602,
     "status": "ok",
     "timestamp": 1575857581012,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "UjsFWCmCDErv",
    "outputId": "3eacfafe-5d8f-4641-dae9-25d2de16a834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#Initializing the neural network\n",
    "\n",
    "nin = trn_term_doc.shape[1] # dimension of input data\n",
    "nh = 100     # number of hidden units\n",
    "nout = 5\n",
    "model = Sequential()\n",
    "model.add(Dense(units=nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(units=nout, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29596,
     "status": "ok",
     "timestamp": 1575857581013,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "9QtTkMQZDgyp",
    "outputId": "c6a80202-f204-4b34-fe12-e4a4c1c6f8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 100)               1704500   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 1,705,005\n",
      "Trainable params: 1,705,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0_v-0VxEEIi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dneirO9cONbq"
   },
   "source": [
    "### Individual Neural Networks for classifying into topics - No Dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_PJE5zjIlZx"
   },
   "outputs": [],
   "source": [
    "# labels = ['Toxic', 'Information', 'Sports', 'Religious', 'Advertisement']\n",
    "# info = {}\n",
    "\n",
    "# vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "#               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "#               smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "# trn_term_doc = vec.fit_transform(df_train[\"Tweet\"])\n",
    "# test_term_doc = vec.transform(df_test[\"Tweet\"])\n",
    "\n",
    "# hists = {}\n",
    "# for label in labels:\n",
    "#   #reset model and create model per label\n",
    "#   K.clear_session()\n",
    "#   nin = trn_term_doc.shape[1] # dimension of input data\n",
    "#   nh = 100  # number of hidden units\n",
    "#   nout = 2 #either 1 or 0 for a label\n",
    "#   model = Sequential()\n",
    "#   model.add(Dense(units=nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "#   model.add(Dense(units=nout, activation='softmax', name='output'))\n",
    "#   opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#   model.compile(optimizer=opt,\n",
    "#                 loss='sparse_categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])\n",
    "\n",
    "#   ytr = df_train[label]\n",
    "#   yts = df_test[label]\n",
    "#   hist = model.fit(trn_term_doc, ytr, epochs=30, batch_size=100, validation_data=(test_term_doc,yts))\n",
    "#   hists[label] = hist\n",
    "#   final_acc = hist.history['val_acc'][-1]\n",
    "#   info[label] = final_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziwwGbAZ8wqd"
   },
   "source": [
    "### Neural Network with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 393947,
     "status": "ok",
     "timestamp": 1575857945377,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "qLi2lcJ7vTai",
    "outputId": "3bdd8492-d5bc-4ed4-9cb1-d4b27e872d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14076 samples, validate on 7039 samples\n",
      "Epoch 1/30\n",
      "14076/14076 [==============================] - 3s 230us/sample - loss: 0.4359 - acc: 0.8171 - val_loss: 0.2589 - val_acc: 0.9115\n",
      "Epoch 2/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.2772 - acc: 0.9091 - val_loss: 0.2175 - val_acc: 0.9115\n",
      "Epoch 3/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.2255 - acc: 0.9153 - val_loss: 0.1795 - val_acc: 0.9175\n",
      "Epoch 4/30\n",
      "14076/14076 [==============================] - 2s 155us/sample - loss: 0.1867 - acc: 0.9295 - val_loss: 0.1417 - val_acc: 0.9270\n",
      "Epoch 5/30\n",
      "14076/14076 [==============================] - 2s 156us/sample - loss: 0.1497 - acc: 0.9439 - val_loss: 0.1063 - val_acc: 0.9497\n",
      "Epoch 6/30\n",
      "14076/14076 [==============================] - 2s 160us/sample - loss: 0.1148 - acc: 0.9592 - val_loss: 0.0813 - val_acc: 0.9663\n",
      "Epoch 7/30\n",
      "14076/14076 [==============================] - 2s 159us/sample - loss: 0.0882 - acc: 0.9709 - val_loss: 0.0641 - val_acc: 0.9757\n",
      "Epoch 8/30\n",
      "14076/14076 [==============================] - 2s 160us/sample - loss: 0.0735 - acc: 0.9749 - val_loss: 0.0527 - val_acc: 0.9811\n",
      "Epoch 9/30\n",
      "14076/14076 [==============================] - 2s 156us/sample - loss: 0.0583 - acc: 0.9814 - val_loss: 0.0455 - val_acc: 0.9835\n",
      "Epoch 10/30\n",
      "14076/14076 [==============================] - 2s 157us/sample - loss: 0.0491 - acc: 0.9858 - val_loss: 0.0391 - val_acc: 0.9859\n",
      "Epoch 11/30\n",
      "14076/14076 [==============================] - 2s 160us/sample - loss: 0.0407 - acc: 0.9887 - val_loss: 0.0354 - val_acc: 0.9872\n",
      "Epoch 12/30\n",
      "14076/14076 [==============================] - 2s 158us/sample - loss: 0.0353 - acc: 0.9882 - val_loss: 0.0322 - val_acc: 0.9882\n",
      "Epoch 13/30\n",
      "14076/14076 [==============================] - 2s 159us/sample - loss: 0.0305 - acc: 0.9901 - val_loss: 0.0294 - val_acc: 0.9898\n",
      "Epoch 14/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0264 - acc: 0.9915 - val_loss: 0.0273 - val_acc: 0.9903\n",
      "Epoch 15/30\n",
      "14076/14076 [==============================] - 2s 160us/sample - loss: 0.0247 - acc: 0.9923 - val_loss: 0.0261 - val_acc: 0.9909\n",
      "Epoch 16/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0206 - acc: 0.9937 - val_loss: 0.0252 - val_acc: 0.9913\n",
      "Epoch 17/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.0182 - acc: 0.9944 - val_loss: 0.0246 - val_acc: 0.9916\n",
      "Epoch 18/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0162 - acc: 0.9958 - val_loss: 0.0229 - val_acc: 0.9926\n",
      "Epoch 19/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0150 - acc: 0.9960 - val_loss: 0.0229 - val_acc: 0.9922\n",
      "Epoch 20/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0118 - acc: 0.9966 - val_loss: 0.0221 - val_acc: 0.9928\n",
      "Epoch 21/30\n",
      "14076/14076 [==============================] - 2s 160us/sample - loss: 0.0120 - acc: 0.9963 - val_loss: 0.0218 - val_acc: 0.9928\n",
      "Epoch 22/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.0114 - acc: 0.9961 - val_loss: 0.0212 - val_acc: 0.9933\n",
      "Epoch 23/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.0206 - val_acc: 0.9935\n",
      "Epoch 24/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0105 - acc: 0.9967 - val_loss: 0.0197 - val_acc: 0.9936\n",
      "Epoch 25/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.0090 - acc: 0.9979 - val_loss: 0.0204 - val_acc: 0.9933\n",
      "Epoch 26/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 0.0200 - val_acc: 0.9935\n",
      "Epoch 27/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0193 - val_acc: 0.9936\n",
      "Epoch 28/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.0070 - acc: 0.9981 - val_loss: 0.0202 - val_acc: 0.9935\n",
      "Epoch 29/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0187 - val_acc: 0.9935\n",
      "Epoch 30/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0057 - acc: 0.9987 - val_loss: 0.0204 - val_acc: 0.9935\n",
      "Train on 14076 samples, validate on 7039 samples\n",
      "Epoch 1/30\n",
      "14076/14076 [==============================] - 2s 171us/sample - loss: 0.5845 - acc: 0.7400 - val_loss: 0.4976 - val_acc: 0.7717\n",
      "Epoch 2/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.5218 - acc: 0.7668 - val_loss: 0.4628 - val_acc: 0.7717\n",
      "Epoch 3/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.4699 - acc: 0.7804 - val_loss: 0.4181 - val_acc: 0.7718\n",
      "Epoch 4/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.4234 - acc: 0.7999 - val_loss: 0.3692 - val_acc: 0.7909\n",
      "Epoch 5/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.3688 - acc: 0.8305 - val_loss: 0.3230 - val_acc: 0.8443\n",
      "Epoch 6/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.3246 - acc: 0.8534 - val_loss: 0.2864 - val_acc: 0.8719\n",
      "Epoch 7/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.2807 - acc: 0.8760 - val_loss: 0.2578 - val_acc: 0.8969\n",
      "Epoch 8/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.2533 - acc: 0.8920 - val_loss: 0.2390 - val_acc: 0.9011\n",
      "Epoch 9/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.2322 - acc: 0.9043 - val_loss: 0.2237 - val_acc: 0.9102\n",
      "Epoch 10/30\n",
      "14076/14076 [==============================] - 2s 174us/sample - loss: 0.2108 - acc: 0.9130 - val_loss: 0.2119 - val_acc: 0.9159\n",
      "Epoch 11/30\n",
      "14076/14076 [==============================] - 2s 177us/sample - loss: 0.1920 - acc: 0.9204 - val_loss: 0.2091 - val_acc: 0.9099\n",
      "Epoch 12/30\n",
      "14076/14076 [==============================] - 3s 181us/sample - loss: 0.1745 - acc: 0.9292 - val_loss: 0.1992 - val_acc: 0.9183\n",
      "Epoch 13/30\n",
      "14076/14076 [==============================] - 2s 173us/sample - loss: 0.1637 - acc: 0.9334 - val_loss: 0.1937 - val_acc: 0.9202\n",
      "Epoch 14/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.1516 - acc: 0.9399 - val_loss: 0.1925 - val_acc: 0.9190\n",
      "Epoch 15/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.1414 - acc: 0.9445 - val_loss: 0.1876 - val_acc: 0.9229\n",
      "Epoch 16/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1314 - acc: 0.9492 - val_loss: 0.1857 - val_acc: 0.9227\n",
      "Epoch 17/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.1251 - acc: 0.9480 - val_loss: 0.1853 - val_acc: 0.9226\n",
      "Epoch 18/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.1154 - acc: 0.9540 - val_loss: 0.1860 - val_acc: 0.9226\n",
      "Epoch 19/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.1086 - acc: 0.9577 - val_loss: 0.1836 - val_acc: 0.9236\n",
      "Epoch 20/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1075 - acc: 0.9575 - val_loss: 0.1838 - val_acc: 0.9231\n",
      "Epoch 21/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.1012 - acc: 0.9587 - val_loss: 0.1840 - val_acc: 0.9230\n",
      "Epoch 22/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0959 - acc: 0.9631 - val_loss: 0.1838 - val_acc: 0.9229\n",
      "Epoch 23/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0924 - acc: 0.9643 - val_loss: 0.1853 - val_acc: 0.9229\n",
      "Epoch 24/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.0849 - acc: 0.9677 - val_loss: 0.1865 - val_acc: 0.9226\n",
      "Epoch 25/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0846 - acc: 0.9680 - val_loss: 0.1892 - val_acc: 0.9229\n",
      "Epoch 26/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.0846 - acc: 0.9661 - val_loss: 0.1892 - val_acc: 0.9240\n",
      "Epoch 27/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0746 - acc: 0.9709 - val_loss: 0.1911 - val_acc: 0.9230\n",
      "Epoch 28/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0740 - acc: 0.9717 - val_loss: 0.1924 - val_acc: 0.9233\n",
      "Epoch 29/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0676 - acc: 0.9756 - val_loss: 0.1917 - val_acc: 0.9233\n",
      "Epoch 30/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0658 - acc: 0.9755 - val_loss: 0.1960 - val_acc: 0.9240\n",
      "Train on 14076 samples, validate on 7039 samples\n",
      "Epoch 1/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.5150 - acc: 0.7884 - val_loss: 0.3966 - val_acc: 0.8393\n",
      "Epoch 2/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.4175 - acc: 0.8358 - val_loss: 0.3495 - val_acc: 0.8393\n",
      "Epoch 3/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.3652 - acc: 0.8522 - val_loss: 0.3029 - val_acc: 0.8503\n",
      "Epoch 4/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.3097 - acc: 0.8704 - val_loss: 0.2570 - val_acc: 0.8702\n",
      "Epoch 5/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.2656 - acc: 0.8927 - val_loss: 0.2129 - val_acc: 0.8993\n",
      "Epoch 6/30\n",
      "14076/14076 [==============================] - 2s 160us/sample - loss: 0.2184 - acc: 0.9116 - val_loss: 0.1837 - val_acc: 0.9163\n",
      "Epoch 7/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1869 - acc: 0.9268 - val_loss: 0.1588 - val_acc: 0.9331\n",
      "Epoch 8/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1629 - acc: 0.9378 - val_loss: 0.1415 - val_acc: 0.9433\n",
      "Epoch 9/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.1419 - acc: 0.9477 - val_loss: 0.1308 - val_acc: 0.9494\n",
      "Epoch 10/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.1253 - acc: 0.9524 - val_loss: 0.1239 - val_acc: 0.9520\n",
      "Epoch 11/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1141 - acc: 0.9564 - val_loss: 0.1136 - val_acc: 0.9578\n",
      "Epoch 12/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1075 - acc: 0.9579 - val_loss: 0.1086 - val_acc: 0.9594\n",
      "Epoch 13/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.0951 - acc: 0.9645 - val_loss: 0.1070 - val_acc: 0.9612\n",
      "Epoch 14/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.0848 - acc: 0.9691 - val_loss: 0.1036 - val_acc: 0.9621\n",
      "Epoch 15/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0829 - acc: 0.9686 - val_loss: 0.1001 - val_acc: 0.9633\n",
      "Epoch 16/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0759 - acc: 0.9717 - val_loss: 0.0972 - val_acc: 0.9641\n",
      "Epoch 17/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.0720 - acc: 0.9733 - val_loss: 0.0973 - val_acc: 0.9649\n",
      "Epoch 18/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0681 - acc: 0.9741 - val_loss: 0.0964 - val_acc: 0.9652\n",
      "Epoch 19/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0635 - acc: 0.9761 - val_loss: 0.0960 - val_acc: 0.9653\n",
      "Epoch 20/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0593 - acc: 0.9781 - val_loss: 0.0964 - val_acc: 0.9653\n",
      "Epoch 21/30\n",
      "14076/14076 [==============================] - 2s 159us/sample - loss: 0.0588 - acc: 0.9773 - val_loss: 0.0928 - val_acc: 0.9665\n",
      "Epoch 22/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0559 - acc: 0.9788 - val_loss: 0.0938 - val_acc: 0.9663\n",
      "Epoch 23/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.0522 - acc: 0.9802 - val_loss: 0.0944 - val_acc: 0.9662\n",
      "Epoch 24/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0511 - acc: 0.9815 - val_loss: 0.0948 - val_acc: 0.9665\n",
      "Epoch 25/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.0465 - acc: 0.9816 - val_loss: 0.0955 - val_acc: 0.9665\n",
      "Epoch 26/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0473 - acc: 0.9824 - val_loss: 0.0914 - val_acc: 0.9680\n",
      "Epoch 27/30\n",
      "14076/14076 [==============================] - 2s 171us/sample - loss: 0.0426 - acc: 0.9836 - val_loss: 0.0930 - val_acc: 0.9679\n",
      "Epoch 28/30\n",
      "14076/14076 [==============================] - 2s 176us/sample - loss: 0.0427 - acc: 0.9844 - val_loss: 0.0940 - val_acc: 0.9676\n",
      "Epoch 29/30\n",
      "14076/14076 [==============================] - 3s 179us/sample - loss: 0.0401 - acc: 0.9847 - val_loss: 0.0918 - val_acc: 0.9683\n",
      "Epoch 30/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0405 - acc: 0.9851 - val_loss: 0.0937 - val_acc: 0.9679\n",
      "Train on 14076 samples, validate on 7039 samples\n",
      "Epoch 1/30\n",
      "14076/14076 [==============================] - 2s 177us/sample - loss: 0.6639 - acc: 0.6507 - val_loss: 0.5590 - val_acc: 0.6831\n",
      "Epoch 2/30\n",
      "14076/14076 [==============================] - 2s 172us/sample - loss: 0.5714 - acc: 0.7065 - val_loss: 0.4957 - val_acc: 0.6951\n",
      "Epoch 3/30\n",
      "14076/14076 [==============================] - 2s 172us/sample - loss: 0.4936 - acc: 0.7619 - val_loss: 0.4091 - val_acc: 0.8257\n",
      "Epoch 4/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.4109 - acc: 0.8202 - val_loss: 0.3325 - val_acc: 0.8754\n",
      "Epoch 5/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.3442 - acc: 0.8564 - val_loss: 0.2762 - val_acc: 0.9051\n",
      "Epoch 6/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.2891 - acc: 0.8833 - val_loss: 0.2390 - val_acc: 0.9145\n",
      "Epoch 7/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.2414 - acc: 0.9069 - val_loss: 0.2097 - val_acc: 0.9270\n",
      "Epoch 8/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.2217 - acc: 0.9119 - val_loss: 0.1921 - val_acc: 0.9307\n",
      "Epoch 9/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1953 - acc: 0.9233 - val_loss: 0.1811 - val_acc: 0.9318\n",
      "Epoch 10/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.1798 - acc: 0.9292 - val_loss: 0.1701 - val_acc: 0.9339\n",
      "Epoch 11/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.1592 - acc: 0.9391 - val_loss: 0.1643 - val_acc: 0.9366\n",
      "Epoch 12/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.1461 - acc: 0.9453 - val_loss: 0.1572 - val_acc: 0.9371\n",
      "Epoch 13/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.1347 - acc: 0.9504 - val_loss: 0.1547 - val_acc: 0.9373\n",
      "Epoch 14/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.1291 - acc: 0.9508 - val_loss: 0.1495 - val_acc: 0.9386\n",
      "Epoch 15/30\n",
      "14076/14076 [==============================] - 2s 163us/sample - loss: 0.1201 - acc: 0.9542 - val_loss: 0.1478 - val_acc: 0.9382\n",
      "Epoch 16/30\n",
      "14076/14076 [==============================] - 2s 161us/sample - loss: 0.1125 - acc: 0.9576 - val_loss: 0.1465 - val_acc: 0.9386\n",
      "Epoch 17/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.1048 - acc: 0.9613 - val_loss: 0.1437 - val_acc: 0.9398\n",
      "Epoch 18/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0978 - acc: 0.9649 - val_loss: 0.1450 - val_acc: 0.9393\n",
      "Epoch 19/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0899 - acc: 0.9670 - val_loss: 0.1430 - val_acc: 0.9396\n",
      "Epoch 20/30\n",
      "14076/14076 [==============================] - 2s 173us/sample - loss: 0.0882 - acc: 0.9677 - val_loss: 0.1413 - val_acc: 0.9403\n",
      "Epoch 21/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.0849 - acc: 0.9687 - val_loss: 0.1461 - val_acc: 0.9403\n",
      "Epoch 22/30\n",
      "14076/14076 [==============================] - 2s 172us/sample - loss: 0.0801 - acc: 0.9726 - val_loss: 0.1410 - val_acc: 0.9413\n",
      "Epoch 23/30\n",
      "14076/14076 [==============================] - 2s 177us/sample - loss: 0.0746 - acc: 0.9737 - val_loss: 0.1404 - val_acc: 0.9420\n",
      "Epoch 24/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0730 - acc: 0.9731 - val_loss: 0.1422 - val_acc: 0.9413\n",
      "Epoch 25/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0680 - acc: 0.9757 - val_loss: 0.1434 - val_acc: 0.9418\n",
      "Epoch 26/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.0655 - acc: 0.9755 - val_loss: 0.1426 - val_acc: 0.9418\n",
      "Epoch 27/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0611 - acc: 0.9776 - val_loss: 0.1456 - val_acc: 0.9423\n",
      "Epoch 28/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0625 - acc: 0.9780 - val_loss: 0.1433 - val_acc: 0.9423\n",
      "Epoch 29/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0573 - acc: 0.9788 - val_loss: 0.1446 - val_acc: 0.9423\n",
      "Epoch 30/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0541 - acc: 0.9804 - val_loss: 0.1447 - val_acc: 0.9422\n",
      "Train on 14076 samples, validate on 7039 samples\n",
      "Epoch 1/30\n",
      "14076/14076 [==============================] - 2s 172us/sample - loss: 0.5828 - acc: 0.7342 - val_loss: 0.4632 - val_acc: 0.7760\n",
      "Epoch 2/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.4829 - acc: 0.7883 - val_loss: 0.4040 - val_acc: 0.7966\n",
      "Epoch 3/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.4155 - acc: 0.8197 - val_loss: 0.3390 - val_acc: 0.8437\n",
      "Epoch 4/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.3484 - acc: 0.8544 - val_loss: 0.2795 - val_acc: 0.8822\n",
      "Epoch 5/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.2942 - acc: 0.8848 - val_loss: 0.2397 - val_acc: 0.8969\n",
      "Epoch 6/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.2522 - acc: 0.9002 - val_loss: 0.2065 - val_acc: 0.9182\n",
      "Epoch 7/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.2162 - acc: 0.9183 - val_loss: 0.1851 - val_acc: 0.9264\n",
      "Epoch 8/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.1898 - acc: 0.9266 - val_loss: 0.1756 - val_acc: 0.9266\n",
      "Epoch 9/30\n",
      "14076/14076 [==============================] - 2s 173us/sample - loss: 0.1667 - acc: 0.9339 - val_loss: 0.1572 - val_acc: 0.9425\n",
      "Epoch 10/30\n",
      "14076/14076 [==============================] - 2s 175us/sample - loss: 0.1529 - acc: 0.9408 - val_loss: 0.1502 - val_acc: 0.9430\n",
      "Epoch 11/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.1461 - acc: 0.9433 - val_loss: 0.1447 - val_acc: 0.9450\n",
      "Epoch 12/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.1264 - acc: 0.9532 - val_loss: 0.1434 - val_acc: 0.9440\n",
      "Epoch 13/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.1198 - acc: 0.9542 - val_loss: 0.1384 - val_acc: 0.9473\n",
      "Epoch 14/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.1108 - acc: 0.9589 - val_loss: 0.1377 - val_acc: 0.9476\n",
      "Epoch 15/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.1043 - acc: 0.9614 - val_loss: 0.1324 - val_acc: 0.9507\n",
      "Epoch 16/30\n",
      "14076/14076 [==============================] - 2s 168us/sample - loss: 0.0970 - acc: 0.9648 - val_loss: 0.1328 - val_acc: 0.9504\n",
      "Epoch 17/30\n",
      "14076/14076 [==============================] - 2s 169us/sample - loss: 0.0941 - acc: 0.9638 - val_loss: 0.1283 - val_acc: 0.9530\n",
      "Epoch 18/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.0887 - acc: 0.9665 - val_loss: 0.1266 - val_acc: 0.9531\n",
      "Epoch 19/30\n",
      "14076/14076 [==============================] - 2s 164us/sample - loss: 0.0852 - acc: 0.9687 - val_loss: 0.1269 - val_acc: 0.9537\n",
      "Epoch 20/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0784 - acc: 0.9706 - val_loss: 0.1288 - val_acc: 0.9527\n",
      "Epoch 21/30\n",
      "14076/14076 [==============================] - 2s 171us/sample - loss: 0.0713 - acc: 0.9741 - val_loss: 0.1274 - val_acc: 0.9534\n",
      "Epoch 22/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.0689 - acc: 0.9738 - val_loss: 0.1270 - val_acc: 0.9541\n",
      "Epoch 23/30\n",
      "14076/14076 [==============================] - 2s 167us/sample - loss: 0.0653 - acc: 0.9778 - val_loss: 0.1317 - val_acc: 0.9528\n",
      "Epoch 24/30\n",
      "14076/14076 [==============================] - 2s 165us/sample - loss: 0.0668 - acc: 0.9753 - val_loss: 0.1265 - val_acc: 0.9541\n",
      "Epoch 25/30\n",
      "14076/14076 [==============================] - 2s 175us/sample - loss: 0.0600 - acc: 0.9776 - val_loss: 0.1264 - val_acc: 0.9544\n",
      "Epoch 26/30\n",
      "14076/14076 [==============================] - 2s 166us/sample - loss: 0.0611 - acc: 0.9782 - val_loss: 0.1265 - val_acc: 0.9544\n",
      "Epoch 27/30\n",
      "14076/14076 [==============================] - 2s 162us/sample - loss: 0.0584 - acc: 0.9789 - val_loss: 0.1280 - val_acc: 0.9543\n",
      "Epoch 28/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.0554 - acc: 0.9805 - val_loss: 0.1256 - val_acc: 0.9543\n",
      "Epoch 29/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.0536 - acc: 0.9799 - val_loss: 0.1287 - val_acc: 0.9544\n",
      "Epoch 30/30\n",
      "14076/14076 [==============================] - 2s 170us/sample - loss: 0.0517 - acc: 0.9818 - val_loss: 0.1273 - val_acc: 0.9535\n"
     ]
    }
   ],
   "source": [
    "labels = ['Toxic', 'Information', 'Sports', 'Religious', 'Advertisement']\n",
    "info = {}\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "              min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "              smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "trn_term_doc = vec.fit_transform(df_train[\"Tweet\"])\n",
    "test_term_doc = vec.transform(df_test[\"Tweet\"])\n",
    "\n",
    "hists = {}\n",
    "for label in labels:\n",
    "  #reset model and create model per label\n",
    "    K.clear_session()\n",
    "    nin = trn_term_doc.shape[1] # dimension of input data\n",
    "    nh = 100  # number of hidden units\n",
    "    nout = 2 #either 1 or 0 for a label\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=nout, activation='softmax', name='output'))\n",
    "    opt = optimizers.Adam(lr=0.001) # beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=opt,\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    ytr = df_train[label]\n",
    "    yts = df_test[label]\n",
    "    hist = model.fit(trn_term_doc, ytr, epochs=30, batch_size=100, validation_data=(test_term_doc,yts))\n",
    "    hists[label] = hist\n",
    "    final_acc = hist.history['val_acc'][-1]\n",
    "    info[label] = final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 393944,
     "status": "ok",
     "timestamp": 1575857945381,
     "user": {
      "displayName": "Michael Li",
      "photoUrl": "",
      "userId": "00224213784858372686"
     },
     "user_tz": 300
    },
    "id": "Z9KBoLPhPpaD",
    "outputId": "ad244576-ad63-4310-dba3-0f2ac7dd803f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic : 0.993465\n",
      "Information : 0.9239949\n",
      "Sports : 0.9678932\n",
      "Religious : 0.94217926\n",
      "Advertisement : 0.95354456\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "\n",
    "for elem in info:\n",
    "    print(elem,\":\",info[elem])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_Neural_Network_Optional_Dropout",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
